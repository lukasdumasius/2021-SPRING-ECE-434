{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECE/CS 434 | MP2: Wi-Fi Fingerprints\n",
    "<br />\n",
    "<nav>\n",
    "    <span class=\"alert alert-block alert-warning\">Due March 14th 11:59PM 2021 on Gradescope</span> |\n",
    "    <a href=\"https://www.gradescope.com/courses/223105\">Gradescope</a> | \n",
    "    <a href=\"https://courses.grainger.illinois.edu/cs434/sp2021/\">Course Website</a> | \n",
    "    <a href=\"http://piazza.com/illinois/spring2021/csece434\">Piazza</a>\n",
    "</nav><br> \n",
    "\n",
    "**Name(s):** _Lukas Dumasius ,NO PARTNER _<br>\n",
    "**NetID(s):** _lukasd2 ,NO PARTNER _\n",
    "\n",
    "<hr />  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "In this MP, you will:\n",
    "- Implement a localization algorithm based on Wi-Fi fingerprints.\n",
    "- Experiment with clustering algorithms such as KNN.\n",
    "- Apply appropriate optimizations to improve localization accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Imports & Setup\n",
    "The following `code` cell, when run, imports the libraries you **might** need for this MP. Feel free to delete or import other commonly used libraries. If Gradescope reports an error and you believe it is due to an unsupported import, check with the TA to see if it could be added. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KDTree\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from statistics import mean \n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem Description\n",
    "There is a room with 3 Wi-Fi access points (their locations are known). You are given the RSSI measurements from all 3 access points (APs) at $N$ unique locations in the room, as well as the phone’s orientation when the measurement was taken. Let’s call this the **“offline phase”**, and call the collected data **\"fingerprints\"**. In the **“online phase”**, a user in the room walks along a **straight** line. Given the RSSI measurements collected by this user’s smartphone and the Wi-Fi fingerprints collected during the off-line phase, **your task is to find the beginning and end locations of this user’s walk**.\n",
    "\n",
    "`offline.csv` contains data for the off-line phase and has 5 columns: `(x, y, alpha, SSID, RSSI)`. \n",
    "\n",
    "`1-walk-*.csv` contain data collected during the on-line phase and has 3 columns: `(SSID, Time, RSSI)`\n",
    "\n",
    "`ap_locations_1.csv` contains the coordinates of each AP. This data is only needed if you want to implement optional optimizations. The reference implementation did not use this data.\n",
    "\n",
    "\n",
    "#### Notes\n",
    "\n",
    "* Refer to `groundtruth.png` for a visual of the setting.\n",
    "* `alpha` is provided in degrees.\n",
    "* `RSSI` is provided in decibels.\n",
    "* `SSID` provides the unique identifier for each AP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Implementation\n",
    "Implement your localization algorithm in the function `wifi_localization(online_file, offline_file, ap_locations)`. Do NOT change its function signature. You are, however, free to define and use helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes three arguments: \n",
    "#    online_file  (string) - name of data file for online phase\n",
    "#    offline_file (string) - name of data file for offline phase\n",
    "#    ap_locations (string) - name of data file for access point locations (optional optimization, reference implementation did not use this)\n",
    "# It returns a list with two tuple values corresponding to the start and end coordinates of the walk\n",
    "\n",
    "def wifi_localization(online_file, offline_file, ap_locations):\n",
    "    \n",
    "    # This is a data wrangling function to organize the offline files into:\n",
    "    # local_list_locs - contains tuples of each location tuple((cur_x, cur_y, cur_alpha)\n",
    "    # local_list_RSSIs - contains tuples of each location tuple((a_RSSI_val, b_RSSI_val, c_RSSI_val)\n",
    "    # it returns these two lists to the main part of the function\n",
    "    # args - none\n",
    "    def location_and_RSSI_collector():\n",
    "        csvfile_fingerprints = pd.read_csv(offline_file)\n",
    "        column_x = csvfile_fingerprints[\"x\"]\n",
    "        column_y = csvfile_fingerprints[\"y\"]\n",
    "        column_alpha = csvfile_fingerprints[\"alpha\"]\n",
    "        column_SSID = csvfile_fingerprints[\"SSID\"]\n",
    "        column_RSSI = csvfile_fingerprints[\"RSSI\"]\n",
    "\n",
    "        local_list_RSSIs = []\n",
    "        local_list_locs = []\n",
    "\n",
    "        cur_x = column_x[0]\n",
    "        cur_y = column_y[0]\n",
    "        cur_alpha = column_alpha[0]\n",
    "        a_RSSI_val = 0\n",
    "        b_RSSI_val = 0\n",
    "        c_RSSI_val = 0\n",
    "        \n",
    "        for i in np.arange(len(column_x)):                 \n",
    "        # len(column_x) = len(column_y) = len(column_alpha) = etc....\n",
    "            if(column_x[i] != cur_x or column_y[i] != cur_y or column_alpha[i] != cur_alpha):  \n",
    "                # we're entering a new (x,y,alpha)\n",
    "                # but first we must store our accumulated values for the previous location (x,y,alpha)\n",
    "                local_list_locs.append(tuple((cur_x, cur_y, cur_alpha)))\n",
    "                local_list_RSSIs.append(tuple((a_RSSI_val, b_RSSI_val, c_RSSI_val)))\n",
    "                # and now we move onto moving onto the next entry:\n",
    "                cur_x = column_x[i]\n",
    "                cur_y = column_y[i]\n",
    "                cur_alpha = column_alpha[i]\n",
    "\n",
    "            # we must do this for all entries (x,y,alpha), new or not               \n",
    "            if(column_SSID[i] == \"a\"):\n",
    "                a_RSSI_val = column_RSSI[i]\n",
    "            if(column_SSID[i] == \"b\"):\n",
    "                b_RSSI_val = column_RSSI[i]\n",
    "            if(column_SSID[i] == \"c\"):\n",
    "                c_RSSI_val = column_RSSI[i]\n",
    "            \n",
    "        # special case for the last one (need to append):\n",
    "        local_list_locs.append(tuple((cur_x, cur_y, cur_alpha)))\n",
    "        local_list_RSSIs.append(tuple((a_RSSI_val, b_RSSI_val, c_RSSI_val)))\n",
    "        return (local_list_locs, local_list_RSSIs)\n",
    "    \n",
    "##########################################################################    \n",
    "    # feel free to replace/modify the next two lines\n",
    "    k_value = 5  # THIS IS HOW MANY NEIGHBORS (In measured/fingerprinted RSSI space) WE AVERAGE ACROSS IN OUR KD TREE\n",
    "                 # In other words, how many K-NN we \"average\" location across to return\n",
    "    \n",
    "    w_value = 3  # THIS IS THE WINDOW's RADIUS. I.E. how many values from either side of current element  in the online data\n",
    "                 # to TRY to grab when averaging the RSSI values across a few elements to reduce effect of outliers\n",
    "                 # this greatly increases the accuracy of the whole algorithm, but you may consider tweaking the values\n",
    "                 # for different data sets.\n",
    "                 # You may increase k_value when you feel that theres many outliers in the offline values.\n",
    "                 # You may increase w_value when you feel that theres many outliers in the online values.\n",
    "                 # that is the basic jist of when you might want to adjust k and w in this implementation\n",
    "                 # some edge cases such as first few and last few elements will not try to derefence non-existant elements, so\n",
    "                 # the window will be a bit smaller there. The linear regression I implemented counteracts most of the effect \n",
    "                 # of the few edge cases with a smaller window size, though.\n",
    "                    \n",
    "    #### getting online info ####\n",
    "    \n",
    "    csvfile_walk = pd.read_csv(online_file)\n",
    "    column_SSID = csvfile_walk[\"SSID\"]\n",
    "    column_time = csvfile_walk[\"Time\"]\n",
    "    column_RSSI = csvfile_walk[\"RSSI\"]\n",
    "    \n",
    "    array_column_SSID = np.array(column_SSID)\n",
    "    array_column_time = np.array(column_time)\n",
    "    array_column_RSSI = np.array(column_RSSI)\n",
    "    \n",
    "    # lets organize the online RSSI a,b,c points into a nice list: list_data_points_online\n",
    "    cur_SSID = array_column_SSID[0]\n",
    "    cur_SSID_indx = 0\n",
    "    list_data_points_online = [[],[],[]]            # 0 = a, 1 = b, 2 = c\n",
    "    for i in np.arange(array_column_SSID.shape[0]):\n",
    "        if(array_column_SSID[i] == \"a\"):\n",
    "            cur_SSID_indx = 0\n",
    "        if(array_column_SSID[i] == \"b\"):\n",
    "            cur_SSID_indx = 1\n",
    "        if(array_column_SSID[i] == \"c\"):\n",
    "            cur_SSID_indx = 2\n",
    "        \n",
    "        list_data_points_online[cur_SSID_indx].append(array_column_RSSI[i])\n",
    "        \n",
    "    # list_data_points_online now contains a,b,c RSSI's from time 0 -> n-1 (each in order)\n",
    "    \n",
    "     #### done - getting online info ####\n",
    "    \n",
    "    list_locs, list_RSSIs = location_and_RSSI_collector()  # list_locs, list_RSSIs corresponding indices are associated. We take advantage of this when querying tree.\n",
    "    array_locs = np.array(list_locs)                       # this is array of tuples (x,y,alpha), actually the tuples are \"array-ized\" too\n",
    "    array_RSSIs = np.array(list_RSSIs)                     # this is array of tuples (a,b,c) corresponding to RSSI vals, actually the tuples are \"array-ized\" too\n",
    "    \n",
    "    # below we leave leaf size, any other things potentially as defaults, shouldnt impact output, just performance\n",
    "    tree = KDTree(array_RSSIs)\n",
    "    \n",
    "    list_windowed_averaged_locations = []                  # these are the final tuples of the walk in form (x, y)\n",
    "    \n",
    "    # for each online data point, lets compute an average RSSI, \n",
    "    # query tree, and get predicted/averaged/windowed location\n",
    "    for i in np.arange(len(list_data_points_online[0])):    \n",
    "        current_avg_RSSI_a = 0\n",
    "        current_avg_RSSI_b = 0\n",
    "        current_avg_RSSI_c = 0\n",
    "        \n",
    "        current_avg_RSSI_a += list_data_points_online[0][i]\n",
    "        current_avg_RSSI_b += list_data_points_online[1][i]\n",
    "        current_avg_RSSI_c += list_data_points_online[2][i]\n",
    "        for k in np.arange(1, w_value + 1):                        # We need a +1 here because np.arange returns [start,stop)\n",
    "            if(i+k < len(list_data_points_online[0])):             # (non-inclusive on last element)\n",
    "                current_avg_RSSI_a += list_data_points_online[0][i+k]\n",
    "                current_avg_RSSI_b += list_data_points_online[1][i+k]\n",
    "                current_avg_RSSI_c += list_data_points_online[2][i+k]\n",
    "            if(i-k >= 0):\n",
    "                current_avg_RSSI_a += list_data_points_online[0][i-k]\n",
    "                current_avg_RSSI_b += list_data_points_online[1][i-k]\n",
    "                current_avg_RSSI_c += list_data_points_online[2][i-k]\n",
    "        current_avg_RSSI_a /= 2*(w_value)+1\n",
    "        current_avg_RSSI_b /= 2*(w_value)+1\n",
    "        current_avg_RSSI_c /= 2*(w_value)+1\n",
    "        \n",
    "        \n",
    "        dist, ind = tree.query([tuple((current_avg_RSSI_a, current_avg_RSSI_b, current_avg_RSSI_c))], k=k_value)\n",
    "        \n",
    "        #### averaging over k neighbor's locations ####\n",
    "        \n",
    "        list_current_neighbors = []\n",
    "        current_avg_x = 0\n",
    "        current_avg_y = 0\n",
    "   \n",
    "        for i in np.arange(k_value):\n",
    "            list_current_neighbors.append(array_locs[ind[0][i]])\n",
    "        for i in np.arange(k_value):\n",
    "            current_avg_x += list_current_neighbors[i][0]  # 0 = x\n",
    "            current_avg_y += list_current_neighbors[i][1]  # 1 = y\n",
    "    \n",
    "        current_avg_x /= k_value\n",
    "        current_avg_y /= k_value\n",
    "        \n",
    "        #### done - averaging over k neighbor's locations ####\n",
    "        \n",
    "        list_windowed_averaged_locations.append(tuple((current_avg_x, current_avg_y)))\n",
    "\n",
    "    num_of_walk_elements = len(list_windowed_averaged_locations)\n",
    "    list_windowed_averaged_locations_just_x = []\n",
    "    list_windowed_averaged_locations_just_y = []\n",
    "    for cur_x, cur_y in list_windowed_averaged_locations:\n",
    "        list_windowed_averaged_locations_just_x.append(cur_x)\n",
    "        list_windowed_averaged_locations_just_y.append(cur_y)\n",
    "    \n",
    "    \n",
    "    #x_axis_n_samples = np.linspace(0, num_of_walk_elements-1, num_of_walk_elements, dtype=int)\n",
    "    x_axis_n_samples = np.arange(0, num_of_walk_elements)\n",
    "    modelx = LinearRegression().fit(np.array(x_axis_n_samples).reshape(-1,1), np.array(list_windowed_averaged_locations_just_x).reshape(-1,1))\n",
    "    modely = LinearRegression().fit(np.array(x_axis_n_samples).reshape(-1,1), np.array(list_windowed_averaged_locations_just_y).reshape(-1,1))\n",
    "    \n",
    "    list_xy_of_interest = [0, num_of_walk_elements-1]        # we are interested in the first (start) and last sample\n",
    "    array_xy_of_interest = np.array(list_xy_of_interest).reshape(-1,1)\n",
    "    \n",
    "    x_final_predicted = modelx.predict(array_xy_of_interest)  # array that contains x_start, x_end\n",
    "    y_final_predicted = modely.predict(array_xy_of_interest)  # array that contains y_start, y_end\n",
    "    \n",
    " \n",
    "    return [tuple((x_final_predicted[0][0],y_final_predicted[0][0])), tuple((x_final_predicted[1][0],y_final_predicted[1][0]))] # Your return value should be in this format: [(x_start, y_start), (x_end, y_end)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Running and Testing\n",
    "The code below runs and evaluates your localization algorithm on the two datasets provided. We will use this same function `estimate_grade(loc, gt)` to grade your MP on the two provided datasets **and on additional (hidden) datasets**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "|  Dataset  |  Expected Output |     Your Output |                  Grade |\n",
       "|:---------:|-----------------:|----------------:|-----------------------:|\n",
       "|     1     |     [(0.0, 2.5), (3.5, 2.5)] | [(0.8717391304347832, 1.7826086956521734), (2.8500000000000005, 2.391304347826086)] |        96.85% |\n",
       "|     2     |     [(0.0, 0.0), (3.0, 3.0)] | [(0.8971428571428568, 1.5657142857142856), (2.602857142857143, 2.314285714285714)] |        93.61% |\n",
       "|   Hidden  |              ??? |             ??? | Graded upon Submission | \n",
       "|   Hidden  |              ??? |             ??? | Graded upon Submission | \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    def estimate_grade(calculated, expected):\n",
    "        X = 0\n",
    "        Y = 1\n",
    "        S = 0\n",
    "        E = 1\n",
    "        calculated = np.array(calculated)\n",
    "        expected = np.array(expected)\n",
    "\n",
    "        # 1. Calculate deviation of walking direction from ground truth\n",
    "        def get_deviation(calculated, expected):\n",
    "            calculated = np.array(calculated[E] - calculated[S])\n",
    "            expected = np.array(expected[E] - expected[S])\n",
    "            with np.errstate(divide='ignore', invalid='ignore'): \n",
    "                dot_prod = np.dot(calculated, expected) / np.linalg.norm(calculated) / np.linalg.norm(expected)\n",
    "                deviation = np.nan_to_num(np.degrees(np.arccos(dot_prod)), nan=90)\n",
    "                return deviation if deviation <= 90 else abs(deviation - 180)\n",
    "\n",
    "        delta_theta = get_deviation(calculated, expected)\n",
    "\n",
    "        # You will receive full points if deviation <= 30 degrees. \n",
    "        # You will receive 0 points if deviation >= 60 degrees.\n",
    "        # Points for deviation between 30 and 60 degrees will be scaled proportionally.\n",
    "        theta_score = 1 if(delta_theta <= 30) else max((1 - abs(delta_theta - 30)/30), 0)\n",
    "\n",
    "        # 2. Calculating absolute distance between calculated and expected S/E coordinates.\n",
    "        dist_errors = expected - calculated\n",
    "        s_dist = np.linalg.norm(dist_errors[S], ord=2)\n",
    "        e_dist = np.linalg.norm(dist_errors[E], ord=2)\n",
    "\n",
    "        # You will receive full points if error <= 0.5 units. \n",
    "        # You will receive 0 points if error >= 3 units.\n",
    "        # Points for error between 0.5 and 3 units will be scaled proportionally.\n",
    "        s_score = 1 if(abs(s_dist) <= 0.5) else max(1 - abs(s_dist - 0.5)/2.5, 0)\n",
    "        e_score = 1 if(abs(e_dist) <= 0.5) else max(1 - abs(e_dist - 0.5)/2.5, 0)\n",
    "        dist_score = (s_score + e_score) / 2\n",
    "\n",
    "        return theta_score * 0.8 + dist_score * 0.2\n",
    "\n",
    "    student_out_1 = wifi_localization(\"1-walk-1.csv\", \"offline.csv\", \"ap_locations_1.csv\")\n",
    "    expected_1 = [(0., 2.5),(3.5, 2.5)]\n",
    "    grade_1 = estimate_grade(student_out_1, expected_1)\n",
    "\n",
    "    student_out_2 = wifi_localization(\"1-walk-2.csv\", \"offline.csv\", \"ap_locations_1.csv\")\n",
    "    expected_2 = [(0., 0.),(3., 3.)]\n",
    "    grade_2 = estimate_grade(student_out_2, expected_2)\n",
    "\n",
    "    from IPython.display import display, Markdown\n",
    "    display(Markdown(\"\"\"\n",
    "|  Dataset  |  Expected Output |     Your Output |                  Grade |\n",
    "|:---------:|-----------------:|----------------:|-----------------------:|\n",
    "|     1     |     {expected_1} | {student_out_1} |        {grade_1:2.2f}% |\n",
    "|     2     |     {expected_2} | {student_out_2} |        {grade_2:2.2f}% |\n",
    "|   Hidden  |              ??? |             ??? | Graded upon Submission | \n",
    "|   Hidden  |              ??? |             ??? | Graded upon Submission | \n",
    "\"\"\".format(\n",
    "        expected_1=expected_1,\n",
    "        student_out_1=student_out_1,\n",
    "        grade_1=(grade_1 * 100),\n",
    "        expected_2=expected_2,\n",
    "        student_out_2=student_out_2,\n",
    "        grade_2=(grade_2 * 100),\n",
    "    )))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Rubric\n",
    "You will be graded on the two walks provided to you (5 points each) and two additional walks under a different setting (15 points each). Make sure you are not over-fitting to the provided data. We will use the same code from the **Running and Testing** section above to grade all 4 traces of data. In plain English, you will be graded on two error metrics:\n",
    "- (80%) How much your walking direction deviates from ground truth. You will receive full points if deviation ≤ 30 degrees, and 0 points if deviation ≥ 60 degrees. Points for deviation between 30 and 60 degrees will be scaled proportionally.\n",
    "- (20%) How much your start and end coordinates deviate from ground truth. You will receive full points if error ≤ 0.5 units. You will receive 0 points if error ≥ 3. Points for error between 0.5 and 3 units will be scaled proportionally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Submission Guidlines\n",
    "This Jupyter notebook is the only file you need to submit on Gradescope. If you are working in a pair, make sure your partner is correctly added on Gradescope and that both of your names are filled in at the top of this file.\n",
    "\n",
    "**Make sure any code you added to this notebook, except for import statements, is either in a function or guarded by `__main__`(which won't be run by the autograder). Gradescope will give you immediate feedback using the provided test cases. It is your responsibility to check the output before the deadline to ensure your submission runs with the autograder.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
